{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10933834-2eba-4833-a0ad-1cd51e1e900f",
   "metadata": {
    "id": "10933834-2eba-4833-a0ad-1cd51e1e900f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b5b1381-afab-4bbb-aecd-5834a430564a",
   "metadata": {
    "id": "1b5b1381-afab-4bbb-aecd-5834a430564a"
   },
   "outputs": [],
   "source": [
    "def div(soup):\n",
    "    \"\"\"\n",
    "    gets all div's from one page\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeatifulSoup object\n",
    "        BeatifulSoup object for a page of results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        a collection of divs for further analysis\n",
    "    \"\"\"\n",
    "    divs=[]\n",
    "    for i in soup.find_all('div',{'class':'s-prose js-post-body'}):\n",
    "        try:\n",
    "            divs.append(i.contents)\n",
    "        except:\n",
    "            divs.append(None)\n",
    "            continue\n",
    "    return divs\n",
    "\n",
    "def href(soup):\n",
    "    \"\"\"\n",
    "    gets all href links from one page\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeatifulSoup object\n",
    "        BeatifulSoup object for a page of results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        a collection of href links for individual thread requests\n",
    "    \"\"\"\n",
    "    hrefs=[]\n",
    "    for i in soup.find_all('a',{'class':'s-link'},href=True):\n",
    "        try:\n",
    "            hrefs.append(i['href'])\n",
    "        except:\n",
    "            hrefs.append(None)\n",
    "            continue\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4127ab79-7320-448c-adb8-a1d204f4bf42",
   "metadata": {
    "id": "4127ab79-7320-448c-adb8-a1d204f4bf42"
   },
   "outputs": [],
   "source": [
    "def clean(hrefs):\n",
    "    \"\"\"\n",
    "    remove all empty lists of hrefs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hrefs : array\n",
    "        a collection of hrefs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        a collection of hrefs without any empty lists\n",
    "    \"\"\"\n",
    "    list_hrefs=[]\n",
    "    for i in hrefs:\n",
    "        if i!=[]:\n",
    "            list_hrefs.append(i)\n",
    "    # merge all elemenets in one list\n",
    "    hrefs_list=[]\n",
    "    for i in list_hrefs:\n",
    "        for j in i:\n",
    "            hrefs_list.append(j)\n",
    "    return hrefs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e70f73-2e49-4a23-98d1-4f1edeedb642",
   "metadata": {
    "id": "c9e70f73-2e49-4a23-98d1-4f1edeedb642"
   },
   "outputs": [],
   "source": [
    "def links(hrefs):\n",
    "    \"\"\"\n",
    "    rearrage those links who do not have 'https://stackoverflow.com' prefix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hrefs : array\n",
    "        a collection of hrefs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        a collection of hrefs with the appropriate domain as a prefix\n",
    "    \"\"\"\n",
    "    new_href=[]\n",
    "    domain='https://stackoverflow.com'\n",
    "    for h in hrefs:\n",
    "        if 'https' not in h:\n",
    "            m=domain+h+\"answertab=votes#tab-top\"\n",
    "            new_href.append(m)\n",
    "        else:\n",
    "            new_href.append(h+\"answertab=votes#tab-top\")\n",
    "    return new_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8e6e46-d897-45ad-899c-88187fa4ec2f",
   "metadata": {
    "id": "4b8e6e46-d897-45ad-899c-88187fa4ec2f"
   },
   "outputs": [],
   "source": [
    "def single(url):\n",
    "    \"\"\"\n",
    "    request a single url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        a url to query with requests.get\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    soup\n",
    "        a BeautifulSoup object for the entire corresponding webpage\n",
    "    \"\"\"\n",
    "    req=requests.get(url=url)\n",
    "    soup=BeautifulSoup(req.text,\"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35657115-d545-41c7-876d-f532401e2c49",
   "metadata": {
    "id": "35657115-d545-41c7-876d-f532401e2c49"
   },
   "outputs": [],
   "source": [
    "def multi(url):\n",
    "    \"\"\"\n",
    "    scrape multiple url's found on this url\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        a url to query with requests.get\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        a collection of questions found as the first <p> tag\n",
    "    array\n",
    "        a collection of answers found as subsequent <p> tags\n",
    "    \"\"\"\n",
    "    page=single(url).find_all('div',{'class':'s-prose js-post-body'})\n",
    "    question=[i.find(\"p\").get_text()for i in page][0]\n",
    "    answer=[i.find(\"p\").get_text() for i in page][1:6]\n",
    "    return question,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37e8fcb-5181-4f21-a55d-480657e5aa52",
   "metadata": {
    "id": "b37e8fcb-5181-4f21-a55d-480657e5aa52"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "def questions_answers(url,start_page,end_page):\n",
    "    \"\"\"\n",
    "    get collections of questions and answers\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        a url to query with requests.get\n",
    "    start_page : int\n",
    "        starting page number to scrape results\n",
    "    end_page : int\n",
    "        ending page number to scrape results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        a collection of raw outputs for each of the question/answer \n",
    "        threads with code tags for further text post-processing\n",
    "    array\n",
    "        a collection of questions found as the first <p> tag\n",
    "    array\n",
    "        a collection of answers found as subsequent <p> tags\n",
    "    \"\"\"\n",
    "    soups=[]\n",
    "    for page in range(start_page,end_page):\n",
    "        req=requests.get(url=url.format(page))\n",
    "        soup=BeautifulSoup(req.text,\"html.parser\")\n",
    "        soups.append(soup)\n",
    "        time.sleep(1)\n",
    "\n",
    "    # obtain all hrefs\n",
    "    hrefs=[]\n",
    "    for soup in soups:\n",
    "        hrefs.append(href(soup))\n",
    "    \n",
    "    # obtain all divs\n",
    "    divs =[]\n",
    "    url='https://stackoverflow.com'\n",
    "    for ref in hrefs:\n",
    "        for link in ref:\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                req=requests.get(url=url+link)\n",
    "            except:\n",
    "                continue\n",
    "        soup=BeautifulSoup(req.text,\"html.parser\")\n",
    "        divs.append(div(soup))\n",
    "\n",
    "    questions=[]\n",
    "    answers=[]\n",
    "    hrefs = clean(hrefs)\n",
    "    hrefs = links(hrefs)\n",
    "\n",
    "    # distinguish questions with answers\n",
    "    for link in hrefs:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            q,a=multi(link)\n",
    "        except:\n",
    "            continue\n",
    "        questions.append(q)\n",
    "        answers.append(a)\n",
    "\n",
    "    # fetch remaining answers to the same question\n",
    "    new=[]\n",
    "    for i in range(len(answers)):\n",
    "        try:\n",
    "            new.append(answers[i][0])\n",
    "        except:\n",
    "            new.append(None)\n",
    "    next_questions=[]\n",
    "    next_answers=[]\n",
    "    merge = list(itertools.chain.from_iterable(answers))\n",
    "    for j in range(len(merge)-1):\n",
    "        next_questions.append(merge[i])\n",
    "        next_answers.append(merge[i+1])\n",
    "\n",
    "    # return questions+next_questions,answers+next_answers\n",
    "    return divs,questions,answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "BB2f_8UCdjDo",
   "metadata": {
    "id": "BB2f_8UCdjDo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0df7c7-5a20-4ced-bae2-5d3e35f759d0",
   "metadata": {
    "id": "7e0df7c7-5a20-4ced-bae2-5d3e35f759d0"
   },
   "outputs": [],
   "source": [
    "all_raw=[]\n",
    "all_q=[]\n",
    "all_a=[]\n",
    "for i in range(1,101,10):\n",
    "    url = 'https://stackoverflow.com/questions/tagged/python?tab=votes&page={}&pagesize=50'\n",
    "    raw,q,a=(questions_answers(url,i,i+10))\n",
    "    all_raw.append(raw)\n",
    "    all_q.append(q)\n",
    "    all_a.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36d000b-55e9-4fcb-8268-9369796d1a4f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "d36d000b-55e9-4fcb-8268-9369796d1a4f",
    "outputId": "7f1321b8-ee8e-4e91-b52e-fbfc30d57182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"raw.csv\", \"w\", encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for raw in all_raw:\n",
    "        for line in raw:\n",
    "            # writer = csv.writer(f)\n",
    "            writer.writerows(line)\n",
    "\n",
    "with open(\"questions.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "    for q in all_q:\n",
    "        for line in q:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "with open(\"answers.csv\", \"w\", encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "        for a in all_a:\n",
    "            # writer = csv.writer(f)\n",
    "            writer.writerows(a)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945e159e-fb77-4af3-abed-d9984c78cbf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "945e159e-fb77-4af3-abed-d9984c78cbf2",
    "outputId": "07df761f-aae0-4aa4-b834-32fb88fb89aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "one2many = []\n",
    "with open(\"qa_pairs.csv\", \"w\", encoding = 'utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"question\", \"answer\"])\n",
    "    for i in range(len(all_q)):\n",
    "        for j in range(len(all_q[i])):\n",
    "            curr_q = all_q[i][j]\n",
    "            one2many.append({'question': curr_q, 'answers': all_a[i][j]})\n",
    "            for curr_a in all_a[i][j]:\n",
    "                writer.writerow([curr_q, curr_a])\n",
    "\n",
    "with open(\"qa_pairs.json\", \"w\") as outfile:\n",
    "    json.dump(one2many, outfile)\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-4V9E6pTHSUY",
   "metadata": {
    "id": "-4V9E6pTHSUY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
